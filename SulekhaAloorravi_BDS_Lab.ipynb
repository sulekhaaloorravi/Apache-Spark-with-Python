{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BDS Assignment Submitted by: Sulekha Aloorravi\n",
    "\n",
    "#Packages to be imported for PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import Row\n",
    "spark = SparkSession.builder.appName(\"Python Lab - Sulekha Aloorravi\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "#Other Python Packages\n",
    "import datetime #To manipulate Time Stamp\n",
    "import re #To use regular expressions for Data clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prerequisites before load data from HDFS to RDD\n",
    "#1. Below variable is used to convert string values of Months into Numeric\n",
    "monthToNum = {\n",
    "        'Jan' : 1,\n",
    "        'Feb' : 2,\n",
    "        'Mar' : 3,\n",
    "        'Apr' : 4,\n",
    "        'May' : 5,\n",
    "        'Jun' : 6,\n",
    "        'Jul' : 7,\n",
    "        'Aug' : 8,\n",
    "        'Sep' : 9, \n",
    "        'Oct' : 10,\n",
    "        'Nov' : 11,\n",
    "        'Dec' : 12 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is to store date and time once it is converted into all numerics\n",
    "def timestamp(x):\n",
    "    return datetime.datetime(int(x[7:11]), #Year\n",
    "                             monthToNum[x[3:6]], #month\n",
    "                             int(x[0:2]), #day\n",
    "                             int(x[12:14]), #hour\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function is a regular expression to store and clean up data and make it suitable for Analysis\n",
    "def comparerecord(input):\n",
    "    \"\"\"Add data as key value pairs\"\"\"\n",
    "    comparison = re.search(reg_exp,input)\n",
    "    if comparison is None:\n",
    "        return (input, 0)\n",
    "    bytes = comparison.group(9)\n",
    "    if bytes == '-':\n",
    "          size = long(0)\n",
    "    else:\n",
    "          size = long(comparison.group(9))            \n",
    "    return (Row(\n",
    "            Host = comparison.group(1),\n",
    "            Hyp1 = comparison.group(2),\n",
    "            Hyp2 = comparison.group(3),\n",
    "            TimeStamp = timestamp(comparison.group(4)),\n",
    "            Get = comparison.group(5) ,\n",
    "            RequestURL = comparison.group(6),\n",
    "            Http = comparison.group(7),\n",
    "            HTTPReplyCode = int(comparison.group(8)),\n",
    "            BytesTransferred = size\n",
    "        ),1)\n",
    "    \n",
    "    #Data in input_rdd is a raw log input file and it needs to be processed into a format that can be used for analysis\n",
    "#The above data seems to follow a pattern and can be parsed and converted using regular expressions\n",
    "reg_exp = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of records discarded: 895\n",
      "Invalid logline: 198.213.130.253 - - [03/Aug/1995:11:29:02 -0400] \"GET /shuttle/missions/sts-34/mission-sts-34.html\"><IMG images/ssbuv1.gif SRC=\"images/small34p.gif/ HTTP/1.0\" 404 -\n",
      "Invalid logline: ztm-13.dial.xs4all.nl - - [04/Aug/1995:09:34:52 -0400] \"GET / /   HTTP/1.0\" 200 7034\n",
      "Invalid logline: pc32.cis.uoguelph.ca - - [04/Aug/1995:10:57:21 -0400] \"GET / /   HTTP/1.0\" 200 7034\n",
      "Invalid logline: sgate08.st-and.ac.uk - - [04/Aug/1995:17:52:59 -0400] \"GET /htbin/wais.pl?Wake Shield HTTP/1.0\" 200 6858\n",
      "Invalid logline: userp2.snowhill.com - - [05/Aug/1995:14:57:06 -0400] \"GET / \" HTTP/1.0\" 200 7034\n",
      "Invalid logline: ppp-nyc-2-64.ios.com - - [05/Aug/1995:20:45:33 -0400] \"GET /shuttle/missions/sts-69/images/images.html 40,207 89,234 HTTP/1.0\" 200 2443\n",
      "Invalid logline: ppp-nyc-2-64.ios.com - - [05/Aug/1995:20:47:52 -0400] \"GET /shuttle/countdown/tour.html 40,243 89,262 HTTP/1.0\" 200 4347\n",
      "Invalid logline: client-71-162.online.apple.com - - [05/Aug/1995:22:53:19 -0400] \"GET /msfc/astro home.html HTTP/1.0\" 404 -\n",
      "Invalid logline: client-71-162.online.apple.com - - [05/Aug/1995:22:53:47 -0400] \"GET /msfc/astro home.html HTTP/1.0\" 404 -\n",
      "Invalid logline: rt99-9.rotterdam.nl.net - - [06/Aug/1995:11:35:40 -0400] \"GET / /   HTTP/1.0\" 200 7034\n",
      "Invalid logline: client-71-69.online.apple.com - - [06/Aug/1995:12:21:18 -0400] \"GET /msfc/astro home.html HTTP/1.0\" 404 -\n",
      "Invalid logline: spark1.ecf.toronto.edu - - [06/Aug/1995:16:13:52 -0400] \"GET / history/apollo/apollo-13/apollo-13.html HTTP/1.0\" 200 7034\n",
      "Invalid logline: bos49.pi.net - - [06/Aug/1995:16:31:40 -0400] \"GET /htbin/wais.pl?satellite pictures HTTP/1.0\" 200 321\n",
      "Invalid logline: 194.151.8.5 - - [07/Aug/1995:08:48:56 -0400] \"GET / /   HTTP/1.0\" 200 7034\n",
      "Invalid logline: www.db.erau.edu - - [07/Aug/1995:11:19:10 -0400] \"GET /robots.txt HTML/1.0 headers\" 404 -\n",
      "Invalid logline: pc532-03.gsfc.nasa.gov - - [07/Aug/1995:12:40:13 -0400] \"GET /facilities/mila.html>Merritt Island Spaceflight Tracking and Data Network Station (MILA)</a>\" 404 -\n",
      "Invalid logline: ara2.acs.muohio.edu - - [07/Aug/1995:20:40:00 -0400] \"GET /history/apollo/apollo-13/apollo1-.html    apollo-1 HTTP/1.0\" 404 -\n",
      "Invalid logline: ara2.acs.muohio.edu - - [07/Aug/1995:20:41:10 -0400] \"GET /history/apollo/apollo-13/apollo1-.html    apollo-1 HTTP/1.0\" 404 -\n",
      "Invalid logline: ara2.acs.muohio.edu - - [07/Aug/1995:20:41:18 -0400] \"GET /history/apollo/apollo-13/apollo1-.html    apollo-1 HTTP/1.0\" 404 -\n",
      "Invalid logline: hyperweb.mit.edu - - [08/Aug/1995:09:01:06 -0400] \"GET /shuttle/technology/sts-newsref/http://www.ksc.nasa.gov/images/shuttle-patch-logo.gif> Table of\" 404 -\n",
      "Read 1569898 lines, successfully parsed 1569003 lines, failed to parse 895 lines\n"
     ]
    }
   ],
   "source": [
    "#Read data from HDFS. I have already placed the input file in HDFS directory and the same is loaded here from HDFS.\n",
    "#This is a function to seggregate records into valid and invalid based on the Regular Expression that was created in \n",
    "#previous function\n",
    "\n",
    "def retrievedata():\n",
    "    \"\"\" Read and Clean up data \"\"\"\n",
    "    input_rdd = (sc\n",
    "             .textFile(\"NASA_access_log_Aug95.gz\")\n",
    "             .map(comparerecord)\n",
    "             .cache())\n",
    "\n",
    "    validrecords = (input_rdd\n",
    "               .filter(lambda x: x[1] == 1)\n",
    "                .map(lambda x: x[0])\n",
    "                .cache())\n",
    "\n",
    "    discardedrecords = (input_rdd\n",
    "                    .filter(lambda x: x[1] == 0)\n",
    "                    .map(lambda x: x[0]))\n",
    "    discardcount = discardedrecords.count()\n",
    "    if discardcount > 0:\n",
    "        print 'No. of records discarded: %d' % discardedrecords.count()\n",
    "        for line in discardedrecords.take(20):\n",
    "            print 'Invalid logline: %s' % line\n",
    "\n",
    "    print 'Read %d lines, valid %d lines, discarded %d lines' % (input_rdd.count(), validrecords.count(), discardedrecords.count())\n",
    "    return input_rdd, validrecords, discardedrecords\n",
    "\n",
    "\n",
    "input_rdd, validrecords, discardedrecords = retrievedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------+\n",
      "|RequestURL                             |count(RequestURL)|\n",
      "+---------------------------------------+-----------------+\n",
      "|/images/NASA-logosmall.gif             |97384            |\n",
      "|/images/KSC-logosmall.gif              |75332            |\n",
      "|/images/MOSAIC-logosmall.gif           |67441            |\n",
      "|/images/USA-logosmall.gif              |67061            |\n",
      "|/images/WORLD-logosmall.gif            |66437            |\n",
      "|/images/ksclogo-medium.gif             |62771            |\n",
      "|/ksc.html                              |43683            |\n",
      "|/history/apollo/images/apollo-logo1.gif|37824            |\n",
      "|/images/launch-logo.gif                |35135            |\n",
      "|/                                      |30327            |\n",
      "+---------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q1: Write spark code( using RDD) to find out top 10 requested URLs along with count of number of times \n",
    "#they have been requested (This information will help company to find out most popular pages and how frequently they are accessed)\n",
    "\n",
    "df = spark.createDataFrame(validrecords)\n",
    "\n",
    "df.registerTempTable(\"df_tab\")\n",
    "spark.sql(\"select RequestURL,count(RequestURL) from \\\n",
    "            df_tab Group by RequestURL order by count(RequestURL) desc limit 10\").cache().show(truncate = False)\n",
    "\n",
    "\n",
    "#This output primarily shows image files which will be pulled when a user tries to access a webpage.\n",
    "#I have written another code in next step to pull only html URLs from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----------------+\n",
      "|RequestURL                                     |count(RequestURL)|\n",
      "+-----------------------------------------------+-----------------+\n",
      "|/ksc.html                                      |43683            |\n",
      "|/shuttle/missions/sts-69/mission-sts-69.html   |24606            |\n",
      "|/shuttle/missions/missions.html                |22451            |\n",
      "|/software/winvn/winvn.html                     |10345            |\n",
      "|/history/history.html                          |10133            |\n",
      "|/history/apollo/apollo.html                    |8985             |\n",
      "|/shuttle/countdown/liftoff.html                |7865             |\n",
      "|/history/apollo/apollo-13/apollo-13.html       |7176             |\n",
      "|/shuttle/technology/sts-newsref/stsref-toc.html|6516             |\n",
      "|/shuttle/missions/sts-69/images/images.html    |5263             |\n",
      "+-----------------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Another code snippet to get response for all html requests alone\n",
    "\n",
    "spark.sql(\"select RequestURL,count(RequestURL) from \\\n",
    "            df_tab where RequestURL like '%.html' Group by RequestURL order by \\\n",
    "            count(RequestURL) desc limit 10\").cache().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|Host                |count(Host)|\n",
      "+--------------------+-----------+\n",
      "|edams.ksc.nasa.gov  |6530       |\n",
      "|piweba4y.prodigy.com|4846       |\n",
      "|163.206.89.4        |4791       |\n",
      "|piweba5y.prodigy.com|4607       |\n",
      "|piweba3y.prodigy.com|4416       |\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q2: Write spark code to find out top 5 hosts / IP making the request along with count (This information will help company \n",
    "#to find out locations where website is popular or to figure out potential DDoS attacks)\n",
    "\n",
    "df2 = spark.createDataFrame(validrecords)\n",
    "\n",
    "df2.registerTempTable(\"df_tab2\")\n",
    "\n",
    "spark.sql(\"select Host, count(Host) from df_tab2 Group by Host \\\n",
    "          order by count(Host) desc limit 5\").cache().show(truncate = False)\n",
    "\n",
    "#This data has a combination of both \"Host names\" and/or \"IP addresses\"\n",
    "#This code is written using SQL Context. Input data is directly retrieved from Spark datafiles without any Database table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|new_TimeStamp|count|\n",
      "+-------------+-----+\n",
      "|1995-08-31 11| 6297|\n",
      "|1995-08-31 10| 6252|\n",
      "|1995-08-31 13| 5948|\n",
      "|1995-08-30 15| 5889|\n",
      "|1995-08-29 15| 5607|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q3: Write spark code to find out top 5 time frame for high traffic (which day of the week or hour of the day receives \n",
    "#peak traffic, this information will help company to manage resources for handling peak traffic load)\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df3 = spark.createDataFrame(validrecords)\n",
    "\n",
    "df3 = df3.withColumn('new_TimeStamp', df3.TimeStamp.substr(1, 13))    \n",
    "\n",
    "df3 = df3.select(\"Host\", \"new_TimeStamp\",\"RequestURL\", \"HTTPReplyCode\", \"BytesTransferred\", \"TimeStamp\")  \n",
    "\n",
    "df3.groupBy(\"new_TimeStamp\").count().sort(desc(\"count\")).limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|new_TimeStamp|count(new_TimeStamp)|\n",
      "+-------------+--------------------+\n",
      "|1995-08-31 11|6297                |\n",
      "|1995-08-31 10|6252                |\n",
      "|1995-08-31 13|5948                |\n",
      "|1995-08-30 15|5889                |\n",
      "|1995-08-29 15|5607                |\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Same result with SQLContext too\n",
    "df3.registerTempTable(\"df_tab3\")\n",
    "spark.sql(\"select new_TimeStamp, count(new_TimeStamp) from df_tab3 Group by new_TimeStamp \\\n",
    "          order by count(new_TimeStamp) desc limit 5\").cache().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|new_TimeStamp|count|\n",
      "+-------------+-----+\n",
      "|1995-08-03 04|   16|\n",
      "|1995-08-03 09|   22|\n",
      "|1995-08-03 05|   43|\n",
      "|1995-08-03 10|   57|\n",
      "|1995-08-03 07|   58|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q4: Write spark code to find out 5 time frames of least traffic (which day of the week or hour of the day receives \n",
    "#least traffic, this information will help company to do production deployment in that time frame so that less number of users \n",
    "#will be affected if some thing goes wrong during deployment)\n",
    "\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "df4 = spark.createDataFrame(validrecords)\n",
    "\n",
    "df4 = df4.withColumn('new_TimeStamp', df4.TimeStamp.substr(1, 13))    \n",
    "\n",
    "df4 = df4.select(\"Host\", \"new_TimeStamp\",\"RequestURL\", \"HTTPReplyCode\", \"BytesTransferred\", \"TimeStamp\")  \n",
    "\n",
    "df4.groupBy(\"new_TimeStamp\").count().sort(asc(\"count\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|new_TimeStamp|count(new_TimeStamp)|\n",
      "+-------------+--------------------+\n",
      "|1995-08-03 04|16                  |\n",
      "|1995-08-03 09|22                  |\n",
      "|1995-08-03 05|43                  |\n",
      "|1995-08-03 10|57                  |\n",
      "|1995-08-03 07|58                  |\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Same result with SQLContext too\n",
    "df4.registerTempTable(\"df_tab4\")\n",
    "spark.sql(\"select new_TimeStamp, count(new_TimeStamp) from df_tab4 Group by new_TimeStamp \\\n",
    "          order by count(new_TimeStamp) asc limit 5\").cache().show(truncate = False)\n",
    "\n",
    "#This result is quite evident since there was a shut down due to hurricane, there were less requests on 03/08/1995 \n",
    "#and people started accessing from the time servers started coming up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|new_TimeStamp|count(new_TimeStamp)|\n",
      "+-------------+--------------------+\n",
      "|1995-08-13 02|266                 |\n",
      "|1995-08-26 05|423                 |\n",
      "|1995-08-13 03|445                 |\n",
      "|1995-08-26 06|479                 |\n",
      "|1995-08-20 06|502                 |\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#So, I have analysed the logs again after excluding 03/08/1995 from TimeStamp\n",
    "spark.sql(\"select new_TimeStamp, count(new_TimeStamp) from df_tab4 where new_TimeStamp not like '1995-08-03%' \\\n",
    "          Group by new_TimeStamp \\\n",
    "          order by count(new_TimeStamp) asc limit 5\").cache().show(truncate = False)\n",
    "\n",
    "#This gives an unbiased response :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Unique HTTP Response codes\n",
      "Count by each code: [(200, 1398207), (302, 26437), (304, 134138), (403, 171), (404, 10020), (501, 27), (500, 3)]\n"
     ]
    }
   ],
   "source": [
    "#Q5: Write spark code to find out unique HTTP codes returned by the server along with count (this information is helpful \n",
    "#for devops team to find out how many requests are failing so that appropriate action can be taken to fix the issue)\n",
    "\n",
    "httpcode = (validrecords\n",
    "                       .map(lambda code: (code.HTTPReplyCode, 1))\n",
    "                       .reduceByKey(lambda x, y : x + y)\n",
    "                       .cache())\n",
    "httpcodelist = httpcode.take(1000)\n",
    "print 'Found %d Unique HTTP Response codes' % len(httpcodelist)\n",
    "print 'Count by each code: %s' % httpcodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In the above response, we can see that HTTP Code: 200 is received by most of the requests. \n",
    "#Code 200 - Request was fulfilled\n",
    "#Code 302 - The data requested actually resides under a different URL, however, the redirection may be altered on occasion.\n",
    "#Code 304 - f the client has done a conditional GET and access is allowed, but the document has not been modified since the \n",
    "#date and time specified \n",
    "#Code 403 - The request is for something forbidden. Authorization will not help.\n",
    "#Code 404 - The server has not found anything matching the URI given\n",
    "#Code 501 - The server does not support the facility required.\n",
    "#Code 500 - The server encountered an unexpected condition which prevented it from fulfilling the request.\n",
    "\n",
    "#This is definitely a very useful information for developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I have used different types of approaches to answer each question.\n",
    "#So far, SQL Context got executed faster compared to RDD and Spark Dataframes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
